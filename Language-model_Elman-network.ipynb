{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJtoMKq53943",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning Course (980)\n",
        "## Assignment Three \n",
        "\n",
        "__Assignment Goals:__\n",
        "\n",
        "- Implementing RNN based language models.\n",
        "- Implementing and applying a Recurrent Neural Network on text classification problem using TensorFlow.\n",
        "- Implementing __many to one__ and __many to many__ RNN sequence processing.\n",
        "\n",
        "In this assignment, you will implement RNN-based language models and compare extracted word representation from different models. You will also compare two different training methods for sequential data: Truncated Backpropagation Through Time __(TBTT)__ and Backpropagation Through Time __(BTT)__. \n",
        "Also, you will be asked to apply Vanilla RNN to capture word representations and solve a text classification problem. \n",
        "\n",
        "\n",
        "__DataSets__: You will use two datasets, an English Literature for language model task (part 1 to 4) and 20Newsgroups for text classification (part 5). \n",
        "\n",
        "\n",
        "1. (30 points) Implement the RNN based language model described by Mikolov et al.[1], also called __Elman network__ and train a language model on the English Literature dataset. This network contains input, hidden and output layer and is trained by standard backpropagation (TBTT with τ = 1) using the cross-entropy loss. \n",
        "   - The input represents the current word while using 1-of-N coding (thus its size is equal to the size of the vocabulary) and vector s(t − 1) that represents output values in the hidden layer from the previous time step. \n",
        "   - The hidden layer is a fully connected sigmoid layer with size 500. \n",
        "   - Softmax Output Layer to capture a valid probability distribution.\n",
        "   - The model is trained with truncated backpropagation through time (TBTT) with τ = 1: the weights of the network are updated based on the error vector computed only for the current time step.\n",
        "   \n",
        "   Download the English Literature dataset and train the language model as described, report the model cross-entropy loss on the train set. Use nltk.word_tokenize to tokenize the documents. \n",
        "For initialization, s(0) can be set to a vector of small values. Note that we are not interested in the *dynamic model* mentioned in the original paper. \n",
        "To make the implementation simpler you can use Keras to define neural net layers, including Keras.Embedding. (Keras.Embedding will create an additional mapping layer compared to the Elman architecture.) \n",
        "\n",
        "2. (20 points) TBTT has less computational cost and memory needs in comparison with *backpropagation through time algorithm (BTT)*. These benefits come at the cost of losing long term dependencies [2]. Now let's try to investigate computational costs and performance of learning our language model with BTT. For training the Elman-type RNN with BTT, one option is to perform mini-batch gradient descent with exactly one sentence per mini-batch. (The input  size will be [1, Sentence Length]). \n",
        "\n",
        "    1. Split the document into sentences (you can use nltk.tokenize.sent_tokenize).\n",
        "    2. For each sentence, perform one pass that computes the mean/sum loss for this sentence; then perform a gradient update for the whole sentence. (So the mini-batch size varies for the sentences with different lengths). You can truncate long sentences to fit the data in memory. \n",
        "    3. Report the model cross-entropy loss.\n",
        "\n",
        "3. (15 points) It does not seem that simple recurrent neural networks can capture truly exploit context information with long dependencies, because of the problem that gradients vanish and exploding. To solve this problem, gating mechanisms for recurrent neural networks were introduced. Try to learn your last model (Elman + BTT) with the SimpleRnn unit replaced with a Gated Recurrent Unit (GRU). Report the model cross-entropy loss. Compare your results in terms of cross-entropy loss with two other approach(part 1 and 2). Use each model to generate 10 synthetic sentences of 15 words each. Discuss the quality of the sentences generated - do they look like proper English? Do they match the training set?\n",
        "    Text generation from a given language model can be done using the following iterative process:\n",
        "   1. Set sequence = \\[first_word\\], chosen randomly.\n",
        "   2. Select a new word based on the sequence so far, add this word to the sequence, and repeat. At each iteration, select the word with maximum probability given the sequence so far. The trained language model outputs this probability. \n",
        "\n",
        "4. (15 points) The text describes how to extract a word representation from a trained RNN (Chapter 4). How we can evaluate the extracted word representation for your trained RNN? Compare the words representation extracted from each of the approaches using one of the existing methods.\n",
        "\n",
        "5. (20 points) We are aiming to learn an RNN model that predicts document categories given its content (text classification). For this task, we will use the 20Newsgroupst dataset. The 20Newsgroupst contains messages from twenty newsgroups.  We selected four major categories (comp, politics, rec, and religion) comprising around 13k documents altogether. Your model should learn word representations to support the classification task. For solving this problem modify the __Elman network__ architecture such that the last layer is a softmax layer with just 4 output neurons (one for each category). \n",
        "\n",
        "    1. Download the 20Newsgroups dataset, and use the implemented code from the notebook to read in the dataset.\n",
        "    2. Split the data into a training set (90 percent) and validation set (10 percent). Train the model on  20Newsgroups.\n",
        "    3. Report your accuracy results on the validation set.\n",
        "\n",
        "__NOTE__: Please use Jupyter Notebook. The notebook should include the final code, results and your answers. You should submit your Notebook in (.pdf or .html) and .ipynb format. (penalty 10 points) \n",
        "\n",
        "To reduce the parameters, you can merge all words that occur less often than a threshold into a special rare token (\\__unk__).\n",
        "\n",
        "__Instructions__:\n",
        "\n",
        "The university policy on academic dishonesty and plagiarism (cheating) will be taken very seriously in this course. Everything submitted should be your own writing or coding. You must not let other students copy your work. Spelling and grammar count.\n",
        "\n",
        "Your assignments will be marked based on correctness, originality (the implementations and ideas are from yourself), clarification and test performance.\n",
        "\n",
        "\n",
        "[1] Tom´ as Mikolov, Martin Kara ˇ fiat, Luk´ ´ as Burget, Jan ˇ Cernock´ ˇ y,Sanjeev Khudanpur: Recurrent neural network based language model, In: Proc. INTERSPEECH 2010\n",
        "\n",
        "[2] Tallec, Corentin, and Yann Ollivier. \"Unbiasing truncated backpropagation through time.\" arXiv preprint arXiv:1705.08209 (2017).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbL5gMwpvKMY",
        "colab_type": "code",
        "outputId": "4cba3935-ae5f-4b40-f41d-d3fb38f350cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from numpy import array\n",
        "import numpy as np\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import SimpleRNN, TimeDistributed, Flatten\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "# source text\n",
        "data_file = \"./datasets/English Literature.txt\"\n",
        "\n",
        "data = open(data_file).read().strip()\n",
        "\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 12633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezkEZB311Ym7",
        "colab_type": "code",
        "outputId": "fe13fd3d-0bf1-4bc8-b07e-d52cf0b61a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "! pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwDUKyo0DwOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_seq(model, tokenizer, seed_text, n_words):\n",
        "    result = []\n",
        "    in_text = seed_text\n",
        "    result.append(seed_text)\n",
        "    resultS = tokenizer.index_word.get(result[len(result) - 1])\n",
        "    for _ in range(n_words):\n",
        "        encoded = model.predict(np.array(result).reshape(1, -1))[0]\n",
        "        encoded = np.log(encoded) \n",
        "        encoded = np.exp(encoded) / np.exp(encoded).sum()\n",
        "        max = np.random.multinomial(1, encoded/(sum(encoded)+0.000001), 1)[0]\n",
        "        encoded = np.argmax(max)\n",
        "        result.append(encoded+1)\n",
        "        # print (tokenizer.index_word.get(result[len(result) - 1]))\n",
        "        resultS = resultS + \" \" + tokenizer.index_word.get(result[len(result) - 1])\n",
        "    return resultS\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF41IcN974_0",
        "colab_type": "text"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "As you can see the acc is about 13% and the perplexity is 209558.2136\n",
        "\n",
        "There is different perplexity defeinition, which you can see my perplexity function below\n",
        "\n",
        "Also, I generated the 10 sentence with size of 15 at the end of this part.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCxmSPbaOSyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = K.pow(np.e, cross_entropy)\n",
        "    return perplexity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL3PMPggH3Kk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0aff303-17be-4cdd-90c6-d60528954c91"
      },
      "source": [
        "sequences = list()\n",
        "for i in range(1, len(encoded)):\n",
        "    sequence = encoded[i-1:i+1]\n",
        "    sequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]\n",
        "\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print (X.shape)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 204088\n",
            "(204088,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM-1uSj6395R",
        "colab_type": "code",
        "outputId": "4748c5da-3d75-457d-e85e-cf2067042f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=None))\n",
        "model.add(SimpleRNN(500, return_sequences=False, activation='sigmoid'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity])\n",
        "\n",
        "model.fit(X, y, epochs=20, verbose=1)\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, None, 50)          631650    \n",
            "_________________________________________________________________\n",
            "simple_rnn_4 (SimpleRNN)     (None, 500)               275500    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 12633)             6329133   \n",
            "=================================================================\n",
            "Total params: 7,236,283\n",
            "Trainable params: 7,236,283\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "204088/204088 [==============================] - 125s 614us/step - loss: 7.0144 - acc: 0.0373 - perplexity: 508471.8186\n",
            "Epoch 2/20\n",
            "204088/204088 [==============================] - 122s 600us/step - loss: 6.4652 - acc: 0.0684 - perplexity: 330994.6917\n",
            "Epoch 3/20\n",
            "204088/204088 [==============================] - 122s 600us/step - loss: 6.2009 - acc: 0.0838 - perplexity: 282566.3783\n",
            "Epoch 4/20\n",
            "204088/204088 [==============================] - 123s 600us/step - loss: 6.0264 - acc: 0.0923 - perplexity: 259396.4218\n",
            "Epoch 5/20\n",
            "204088/204088 [==============================] - 122s 600us/step - loss: 5.8920 - acc: 0.0994 - perplexity: 249573.2107\n",
            "Epoch 6/20\n",
            "204088/204088 [==============================] - 122s 600us/step - loss: 5.7741 - acc: 0.1051 - perplexity: 242885.8732\n",
            "Epoch 7/20\n",
            "204088/204088 [==============================] - 122s 600us/step - loss: 5.6690 - acc: 0.1091 - perplexity: 236658.1321\n",
            "Epoch 8/20\n",
            "204088/204088 [==============================] - 122s 599us/step - loss: 5.5712 - acc: 0.1140 - perplexity: 231198.0541\n",
            "Epoch 9/20\n",
            "204088/204088 [==============================] - 123s 600us/step - loss: 5.4782 - acc: 0.1176 - perplexity: 226116.7608\n",
            "Epoch 10/20\n",
            "204088/204088 [==============================] - 122s 600us/step - loss: 5.3895 - acc: 0.1201 - perplexity: 222303.3476\n",
            "Epoch 11/20\n",
            "204088/204088 [==============================] - 122s 599us/step - loss: 5.3017 - acc: 0.1229 - perplexity: 218341.8198\n",
            "Epoch 12/20\n",
            "204088/204088 [==============================] - 122s 599us/step - loss: 5.2176 - acc: 0.1252 - perplexity: 215447.2210\n",
            "Epoch 13/20\n",
            "204088/204088 [==============================] - 122s 600us/step - loss: 5.1390 - acc: 0.1266 - perplexity: 213312.6110\n",
            "Epoch 14/20\n",
            "204088/204088 [==============================] - 123s 604us/step - loss: 5.0687 - acc: 0.1273 - perplexity: 211621.9710\n",
            "Epoch 15/20\n",
            "204088/204088 [==============================] - 123s 602us/step - loss: 5.0051 - acc: 0.1288 - perplexity: 210122.3724\n",
            "Epoch 16/20\n",
            "204088/204088 [==============================] - 125s 611us/step - loss: 4.9500 - acc: 0.1301 - perplexity: 208972.2680\n",
            "Epoch 17/20\n",
            "204088/204088 [==============================] - 125s 612us/step - loss: 4.9028 - acc: 0.1303 - perplexity: 207511.2332\n",
            "Epoch 18/20\n",
            "204088/204088 [==============================] - 125s 611us/step - loss: 4.8623 - acc: 0.1307 - perplexity: 206112.3444\n",
            "Epoch 19/20\n",
            "204088/204088 [==============================] - 125s 611us/step - loss: 4.8281 - acc: 0.1310 - perplexity: 204758.9201\n",
            "Epoch 20/20\n",
            "204088/204088 [==============================] - 125s 611us/step - loss: 4.7990 - acc: 0.1311 - perplexity: 203454.3845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f26389b5588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWnbERtRLHS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"model.h5\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1FZSmsXETwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "c06a14a9-4dde-480e-c1c9-923a3b76989a"
      },
      "source": [
        "print (\"First model:\")\n",
        "for i in range(0, 10):  \n",
        "  print (i + 1)\n",
        "  print(generate_seq(model, tokenizer, random.randrange(vocab_size), 15))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First model:\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "forbidden surgeon can like so thing us i thee i gloucester thou which him rash awaking\n",
            "2\n",
            "time's lucio are i fair what baptista of are shall draw and what thee not given\n",
            "3\n",
            "invocate will thy art him of and art captain to clear wearers would and unless on\n",
            "4\n",
            "bestride and gloucester where do my catesby see our could like security my by so to\n",
            "5\n",
            "reproof to them an awhile go then near thou scold to love thou drops in he\n",
            "6\n",
            "request mercy and both jest please face kate you do me richard whose legs that going\n",
            "7\n",
            "cowardly is actions was first now queen i as have behoveful with it may for him\n",
            "8\n",
            "although of and is all no shepherd shall claudio artificial dost go cannot hands of were\n",
            "9\n",
            "braves you work my now of be he'll stay'd this souls is sets be gold have\n",
            "10\n",
            "employment against and lived a side of burgundy to and go spend with clifford to slaves\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YQ89WCCFp6G",
        "colab_type": "text"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "As you can see these are the results:\n",
        "\n",
        "Loss: {4.692541230735192}\n",
        "\n",
        "ACC: {0.15408897705967708}\n",
        "\n",
        "perplexity: {256945.7147414976}\n",
        "\n",
        "Also, I generated the 10 sentence with size of 15 at the end of this part.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9YaMS6D395Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from itertools import chain \n",
        "\n",
        "\n",
        "data_file = \"./datasets/English Literature.txt\"\n",
        "\n",
        "data = open(data_file).read().strip()\n",
        "\n",
        "\n",
        "encoded = sent_tokenize(data)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "sequencesX = []\n",
        "sequencesY = []\n",
        "\n",
        "for i in range (0, len(encoded)):\n",
        "    tokenizer.fit_on_texts([encoded[i]])\n",
        "    words = tokenizer.texts_to_sequences([encoded[i]])[0]\n",
        "    if (len(words) <= 1):\n",
        "        continue\n",
        "    sequence = words[0:len(words) - 1]\n",
        "    sequencesX.append(sequence)\n",
        "    sequence = words[1:len(words)]\n",
        "    sequencesY.append(sequence)\n",
        "\n",
        "sequencesX = array(sequencesX)\n",
        "sequencesY = array(sequencesY)\n",
        "\n",
        "X = sequencesX\n",
        "y = sequencesY\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM4wxDreqd1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhy8GR_Wpph2",
        "colab_type": "code",
        "outputId": "b44fcacf-a888-4eb8-a8a5-06ecd51bf1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocab_size, 50, input_length=None))\n",
        "model2.add(SimpleRNN(500, return_sequences=False, activation='sigmoid'))\n",
        "model2.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model2.summary())\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "\n",
        "for e in range(20):\n",
        "    print(\"Epoch:\" , (e + 1))\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    per = 0\n",
        "    c = 0\n",
        "    for i in range(len(X)):\n",
        "        tmp = np.array(X[i])\n",
        "        sequenceY=[]\n",
        "        sequenceX=[]\n",
        "        if(len(y[i]) < 1):\n",
        "            continue\n",
        "        else:\n",
        "            for j in range(len(y[i])):\n",
        "                sequenceY.append(y[i][j])\n",
        "            for j in range(len(np.array(X[i]))):\n",
        "                sequenceX.append(tmp[j])\n",
        "            sequenceX = np.array(sequenceX)\n",
        "            y = to_categorical(sequenceY, num_classes=vocab_size)\n",
        "            history = model2.fit(sequenceX, y,verbose=0, batch_size=len(tmp),  epochs=1)\n",
        "            loss += history.history['loss'][0]\n",
        "            acc += history.history['acc'][0]\n",
        "            per += history.history['perplexity'][0]\n",
        "            c += 1\n",
        "    print(\"Loss:\", {loss / c})\n",
        "    print(\"ACC:\", {acc / c})\n",
        "    print(\"perplexity:\", {per / c})"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_21 (Embedding)     (None, None, 50)          631650    \n",
            "_________________________________________________________________\n",
            "simple_rnn_20 (SimpleRNN)    (None, 500)               275500    \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 12633)             6329133   \n",
            "=================================================================\n",
            "Total params: 7,236,283\n",
            "Trainable params: 7,236,283\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch: 1\n",
            "Loss: {7.221953507156118}\n",
            "ACC: {0.03287501753406422}\n",
            "perplexity: {815854.147357508}\n",
            "Epoch: 2\n",
            "Loss: {6.604992292591675}\n",
            "ACC: {0.06046957436715353}\n",
            "perplexity: {436012.5156722426}\n",
            "Epoch: 3\n",
            "Loss: {6.2889178734597975}\n",
            "ACC: {0.08113147898301172}\n",
            "perplexity: {361909.70728899864}\n",
            "Epoch: 4\n",
            "Loss: {6.0862827845805265}\n",
            "ACC: {0.09303785805825926}\n",
            "perplexity: {339181.39977692184}\n",
            "Epoch: 5\n",
            "Loss: {5.934108002208649}\n",
            "ACC: {0.10164666728461774}\n",
            "perplexity: {320702.94676213}\n",
            "Epoch: 6\n",
            "Loss: {5.798814981842111}\n",
            "ACC: {0.1095693442842499}\n",
            "perplexity: {311503.06019980705}\n",
            "Epoch: 7\n",
            "Loss: {5.678454449500058}\n",
            "ACC: {0.11619610427804739}\n",
            "perplexity: {303699.7124116557}\n",
            "Epoch: 8\n",
            "Loss: {5.5702285486811824}\n",
            "ACC: {0.12054707892227003}\n",
            "perplexity: {297999.4676011305}\n",
            "Epoch: 9\n",
            "Loss: {5.4668367483793086}\n",
            "ACC: {0.1248644637243193}\n",
            "perplexity: {294062.396947572}\n",
            "Epoch: 10\n",
            "Loss: {5.368668587737484}\n",
            "ACC: {0.12867780129949077}\n",
            "perplexity: {290401.8573646008}\n",
            "Epoch: 11\n",
            "Loss: {5.270730622269936}\n",
            "ACC: {0.13248527760223566}\n",
            "perplexity: {287286.91057256045}\n",
            "Epoch: 12\n",
            "Loss: {5.173504061525381}\n",
            "ACC: {0.13626969607482842}\n",
            "perplexity: {282087.7596556061}\n",
            "Epoch: 13\n",
            "Loss: {5.07494286844233}\n",
            "ACC: {0.1396345667754436}\n",
            "perplexity: {277865.159245155}\n",
            "Epoch: 14\n",
            "Loss: {4.9862358579239805}\n",
            "ACC: {0.14360149807891578}\n",
            "perplexity: {273362.11803937424}\n",
            "Epoch: 15\n",
            "Loss: {4.910687249394467}\n",
            "ACC: {0.14609130506661588}\n",
            "perplexity: {269631.5985222319}\n",
            "Epoch: 16\n",
            "Loss: {4.848008271559459}\n",
            "ACC: {0.1486373909123977}\n",
            "perplexity: {265951.1091816419}\n",
            "Epoch: 17\n",
            "Loss: {4.796977080581592}\n",
            "ACC: {0.1500893994823947}\n",
            "perplexity: {262906.2538039807}\n",
            "Epoch: 18\n",
            "Loss: {4.75436391521391}\n",
            "ACC: {0.15214498976996083}\n",
            "perplexity: {260393.0444511483}\n",
            "Epoch: 19\n",
            "Loss: {4.720603500123183}\n",
            "ACC: {0.15341168680350414}\n",
            "perplexity: {258684.85136085394}\n",
            "Epoch: 20\n",
            "Loss: {4.692541230735192}\n",
            "ACC: {0.15408897705967708}\n",
            "perplexity: {256945.7147414976}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNqAuWkymsDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.save(\"model2.h5\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('model2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKyB7N2g395c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5c4b1eec-7cd8-40d5-a5e0-0e61be229523"
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocab_size, 50, input_length=None))\n",
        "model2.add(SimpleRNN(500, return_sequences=False, activation='sigmoid'))\n",
        "model2.add(Dense(vocab_size, activation='softmax'))\n",
        "def loadModel():\n",
        "  model2.load_weights(\"./model2 (1).h5\")\n",
        "\n",
        "loadModel()\n",
        "print (\"Second model:\")\n",
        "for i in range(0, 10): \n",
        "  print(generate_seq(model2, tokenizer, random.randrange(vocab_size), 15))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Second model:\n",
            "whores to were and of and therefore play king thou thanks florizel sit to noble what's\n",
            "stomachers lodowick to strike york and against my affliction not pompey allowed is that provost an\n",
            "trebles in belly not well though so or all in he both from he not wilt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "paris' were thou sign it on ere be scandal you take a lady they ear me\n",
            "excuses to and these elizabeth my richmond to it on awhile to apparel to if be\n",
            "temporizer in faced to that stumble to your faith to your from henry farewell from menenius\n",
            "shoe changed read he what ha gave it horse i late of huntsman be fortunes hit\n",
            "nicety to hither did forward brutus go i keep if of his hence to sir cannot\n",
            "disorder seat my done this empty elizabeth were to love my state they us king you\n",
            "kernel he in here came duke made to if him love thou dark nursed reprieves with\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk3B1HKmGHUi",
        "colab_type": "text"
      },
      "source": [
        "## Question 3:\n",
        "\n",
        "As you can see we got this results:\n",
        "\n",
        "Loss: {4.34799251055036}\n",
        "\n",
        "ACC: {0.16645361445349946}\n",
        "\n",
        "perplexity: {121986.61053915822}\n",
        "\n",
        "As you can see the loss of this model with GRU is less than the models we used in Q1 and Q2.\n",
        "\n",
        "You can see the generated sentences for all three models at the end of this part.\n",
        "\n",
        "the quality of sentences from the GRU model is better as we expect.\n",
        "\n",
        "The sentences in general is a little similar to English, They make sense besides each other but we can't count them as a meaningful sentence for sure. for having better generated sentences we need to increase the numner of epochs which I was unable due to limitations in resources.\n",
        "But they are kind of similar to the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R08LDjGb395e",
        "colab_type": "code",
        "outputId": "a578c993-1bc5-4f08-874c-8935f05ba92c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from itertools import chain \n",
        "from keras.layers import GRU\n",
        "\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(vocab_size, 50, input_length=None))\n",
        "model3.add(GRU(500, return_sequences=False, activation='sigmoid'))\n",
        "model3.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model3.summary())\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity])\n",
        "for e in range(20):\n",
        "    print(\"Epoch:\" , (e + 1))\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    per = 0\n",
        "    c = 0\n",
        "    for i in range(len(X)):\n",
        "        tmp = np.array(X[i])\n",
        "        sequenceY=[]\n",
        "        sequenceX=[]\n",
        "        if(len(y[i]) < 1):\n",
        "            continue\n",
        "        else:\n",
        "            for j in range(len(y[i])):\n",
        "                sequenceY.append(y[i][j])\n",
        "            for j in range(len(np.array(X[i]))):\n",
        "                sequenceX.append(tmp[j])\n",
        "            sequenceX = np.array(sequenceX)\n",
        "            y = to_categorical(sequenceY, num_classes=vocab_size)\n",
        "            history = model3.fit(sequenceX, y,verbose=0, batch_size=len(tmp),  epochs=1)\n",
        "            loss += history.history['loss'][0]\n",
        "            acc += history.history['acc'][0]\n",
        "            per += history.history['perplexity'][0]\n",
        "            c += 1\n",
        "    print(\"Loss:\", {loss / c})\n",
        "    print(\"ACC:\", {acc / c})\n",
        "    print(\"perplexity:\", {per / c})"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_20 (Embedding)     (None, None, 50)          631650    \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 500)               826500    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 12633)             6329133   \n",
            "=================================================================\n",
            "Total params: 7,787,283\n",
            "Trainable params: 7,787,283\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch: 1\n",
            "Loss: {7.015517119945779}\n",
            "ACC: {0.034783006615057725}\n",
            "perplexity: {447143.91879701643}\n",
            "Epoch: 2\n",
            "Loss: {6.416574023560013}\n",
            "ACC: {0.06383896742052501}\n",
            "perplexity: {196616.564725189}\n",
            "Epoch: 3\n",
            "Loss: {6.0673916762581666}\n",
            "ACC: {0.08300309828349565}\n",
            "perplexity: {167324.09997899493}\n",
            "Epoch: 4\n",
            "Loss: {5.8144855614327176}\n",
            "ACC: {0.09624678261184515}\n",
            "perplexity: {157557.04093382895}\n",
            "Epoch: 5\n",
            "Loss: {5.581344340053405}\n",
            "ACC: {0.10611336651537477}\n",
            "perplexity: {152211.264178411}\n",
            "Epoch: 6\n",
            "Loss: {5.387094496495729}\n",
            "ACC: {0.11532932407631068}\n",
            "perplexity: {149384.67047216103}\n",
            "Epoch: 7\n",
            "Loss: {5.2173552844420685}\n",
            "ACC: {0.12387432635194116}\n",
            "perplexity: {142623.45750503778}\n",
            "Epoch: 8\n",
            "Loss: {5.0545352310890195}\n",
            "ACC: {0.13112159776983492}\n",
            "perplexity: {139299.61038143997}\n",
            "Epoch: 9\n",
            "Loss: {4.922358070634105}\n",
            "ACC: {0.13753882125978253}\n",
            "perplexity: {136217.18549240514}\n",
            "Epoch: 10\n",
            "Loss: {4.7981360870123595}\n",
            "ACC: {0.14422745000346116}\n",
            "perplexity: {131686.40822570614}\n",
            "Epoch: 11\n",
            "Loss: {4.687656569863672}\n",
            "ACC: {0.15019187861382455}\n",
            "perplexity: {129144.50268468693}\n",
            "Epoch: 12\n",
            "Loss: {4.6028016651643915}\n",
            "ACC: {0.15475010431088393}\n",
            "perplexity: {128014.5871994838}\n",
            "Epoch: 13\n",
            "Loss: {4.538741539373824}\n",
            "ACC: {0.15919373454891825}\n",
            "perplexity: {126778.49594608191}\n",
            "Epoch: 14\n",
            "Loss: {4.486341566209225}\n",
            "ACC: {0.1616531413513109}\n",
            "perplexity: {125373.51687362301}\n",
            "Epoch: 15\n",
            "Loss: {4.442514466862913}\n",
            "ACC: {0.16388105957384055}\n",
            "perplexity: {124589.5696204519}\n",
            "Epoch: 16\n",
            "Loss: {4.412345023505946}\n",
            "ACC: {0.16520677742444606}\n",
            "perplexity: {123966.52984428575}\n",
            "Epoch: 17\n",
            "Loss: {4.383090632167992}\n",
            "ACC: {0.16580033807264727}\n",
            "perplexity: {123572.38723129846}\n",
            "Epoch: 18\n",
            "Loss: {4.367944762183847}\n",
            "ACC: {0.1657876830168083}\n",
            "perplexity: {123055.89458969755}\n",
            "Epoch: 19\n",
            "Loss: {4.354770422856786}\n",
            "ACC: {0.16581755733479894}\n",
            "perplexity: {122511.41453893337}\n",
            "Epoch: 20\n",
            "Loss: {4.34799251055036}\n",
            "ACC: {0.16645361445349946}\n",
            "perplexity: {121986.61053915822}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGnAQ5Onm3at",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3.save(\"model33.h5\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('model33.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLQdfwPK84cU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "c788647b-b239-49f9-c150-c1e41f007051"
      },
      "source": [
        "\n",
        "print (\"Third model:\")\n",
        "for i in range(0, 10):  \n",
        "  print (i + 1)\n",
        "  print(generate_seq(model3, tokenizer, random.randrange(vocab_size), 15))\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Third model:\n",
            "1\n",
            "market knave i preys lasted callest standard ethiopian's grissel unsway'd altitude pinch captum opportunity spicery minimo\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sky and reprobate screen happened knowist baptized blades adopts waned admit gardener stabbing unbraided feathers crook'd\n",
            "3\n",
            "innocent power of distinctly mustering slop stew cement effects hoarded pother pinion'd vulgarly suffolk walled shaken\n",
            "4\n",
            "royalties have unvalued forgets feather'd unpossible covenants crassus dishonourable sayings hawthorn i'm est creating disobedient tumble\n",
            "5\n",
            "nobility to i as lucky woodstock's connive disclaiming misleading make's predominant predominant subjected emmew 'point hoarded\n",
            "6\n",
            "dulcet to to tuns burthenous businesses nursing separation wing'd burthenous poland believing nineteen banners courtiers' chapless\n",
            "7\n",
            "carpets worship distant fair'st crickets uplifted nursing chivalrous inclinest machiavel court'sy pinion'd unmusical confess'd callest grandchild\n",
            "8\n",
            "glut so fustian filth 'priami separation cropp'd dresser wards confusions consolation quoifs properties reform'd ebb'd laudis\n",
            "9\n",
            "horseman which which prizes connive burthenous poesy emboss'd tenement allowance talkest gnat cramps grazing 'beseech wakened\n",
            "10\n",
            "incline shame squire whencesoever signor curster supplication wizard coffin'd buttery family jewry steed's dribbling wounding ending\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cpR8n7M395g",
        "colab_type": "code",
        "outputId": "91d5a168-33be-4bbf-95ed-2ea625e66718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"First model:\")\n",
        "for i in range(0, 10):  \n",
        "  print (i + 1)\n",
        "  print(generate_seq(model, tokenizer, random.randrange(vocab_size) , 15))\n",
        "\n",
        "print (\"Second model:\")\n",
        "for i in range(0, 10): \n",
        "  print (i + 1)\n",
        "  print(generate_seq(model2, tokenizer, random.randrange(vocab_size), 15))\n",
        "\n",
        "print (\"Third model:\")\n",
        "for i in range(0, 10):  \n",
        "  print (i + 1)\n",
        "  print(generate_seq(model3, tokenizer, random.randrange(vocab_size), 15))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First model:\n",
            "1\n",
            "hoop i her again brother should courage my red worth to his tear to he sire\n",
            "2\n",
            "rod i at his we grow that nay no devil on he her which us me\n",
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tractable i warwick take it not dove thou enough to clap brutus shall than him o\n",
            "4\n",
            "vilely pawn is may thee jewel you framed o beat my nor question to if my\n",
            "5\n",
            "traditional limbs strange friends and sea excels king o or and let make better we both\n",
            "6\n",
            "unwittingly which richard whose doxy warrant time you body i they time i by thinks in\n",
            "7\n",
            "abominable him draw are stands with so thee provost where my norfolk queen camillo gone of\n",
            "8\n",
            "snakes is henry farewell live should term duke is witch tunis o all here this is\n",
            "9\n",
            "fume you me to whom him i loving now whom him clarence all a mind thy\n",
            "10\n",
            "bail york know it at that than bear myself brutus eye ill what me of report\n",
            "Second model:\n",
            "1\n",
            "herein wipe it they was this go name not edward that true to holy strike place\n",
            "2\n",
            "volscians men king what present arms tent on if have own wife them blood be night\n",
            "3\n",
            "pitied'st bitter were katharina all heaven and a instruction my nor your abroad me warwick world\n",
            "4\n",
            "incapable you gracious now a brow thou prince heard and of a gentleman you i hath\n",
            "5\n",
            "boughs us king my beseeming it part shall of burn if have thine or all and\n",
            "6\n",
            "florence to your bed edward know so by a blow they bore so making to needless\n",
            "7\n",
            "'sir all you may fare sin neat more restraint yet my tell it not still a\n",
            "8\n",
            "awaken but richard would noise truth to march of be brother's did my services may love\n",
            "9\n",
            "overthrow duke made make if upon make figure you her gods of show'd make if how\n",
            "10\n",
            "accident of abhorson take so to his pine go murder think where on foot my could\n",
            "Third model:\n",
            "1\n",
            "king' accusation with to loan knew'st drenched repast triton burthenous uncomfortable let'st roan pinion'd junkets 'sigeia\n",
            "2\n",
            "afford all all that tormenting fixture paste plunged sworest parlor expiration fittest xanthippe unpossible suppresseth rejourn\n",
            "3\n",
            "admit all of he kinder accountant grudges whetted imposter numbness short'st bedeck stomach careful emboss'd helmets\n",
            "4\n",
            "incidency him him trenchers dearest interrupts disarm freshly hate's found'st descending poetry divinest attending unhoped hobby\n",
            "5\n",
            "front a effects yonder's hotly friars characters dresser vowed germane haberdasher tuae adultery rumour'd virtue's monster's\n",
            "6\n",
            "bugle blow you is and and stint hoa chidest shivers buttery moi settle skirts peepeth perfectly\n",
            "7\n",
            "maim thou schoolboys' prolong tossing virtue's wanteth fixture forgetting contended pinion'd feelingly poise quicksand dice brotherly\n",
            "8\n",
            "agrees me ignomy tossing 'faith 'coriolanus deluge venuto stalk fealty generations 'lay roan vulgarly impairing insolent\n",
            "9\n",
            "precursors set hate's burgher maidenheads steep giant haunts concerning narrowly buildings tormenting spaniel overset joiner marcheth\n",
            "10\n",
            "melt and and me unvalued skein footboy monachum toaze solum xanthippe icarus tongued playfellows trinkets tempter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIJduQf9Qo-e",
        "colab_type": "text"
      },
      "source": [
        "## Question 4:\n",
        "\n",
        "For this part we are going to do this steps:\n",
        "\n",
        "1_ load the model of Q1 and Q2\n",
        "\n",
        "2_ replace the weights of the first layer of Q1 model with the first layer weights of Q2 model\n",
        "\n",
        "3_ run the evaluate function for the new model\n",
        "\n",
        "4_ do these steps for any possible pair of modesl.\n",
        "\n",
        "### Result:\n",
        "\n",
        "As you can see:\n",
        "\n",
        "model Q1 with first layer of model Q1 got loss = 4.6\n",
        "\n",
        "model Q1 with first layer of model Q2 got loss = 9.7\n",
        "\n",
        "model Q1 with first layer of model Q3 got loss = 8.3\n",
        "\n",
        "_\n",
        "\n",
        "model Q2 with first layer of model Q2 got loss = 5.7\n",
        "\n",
        "model Q2 with first layer of model Q1 got loss = 12.4\n",
        "\n",
        "model Q2 with first layer of model Q3 got loss = 8.4\n",
        "\n",
        "_\n",
        "\n",
        "model Q3 with first layer of model Q3 got loss = 5.7\n",
        "\n",
        "model Q3 with first layer of model Q2 got loss = 11.6\n",
        "\n",
        "model Q3 with first layer of model Q1 got loss = 13.1\n",
        "\n",
        "As you can see here we got lower loss with using the first layer of the model from Q3 (8.3, 8.4, 5.7), therefore, this model is better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzy9aiewQsSe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c8d937eb-e83f-435e-f5ee-f367ea5c1780"
      },
      "source": [
        "model.load_weights(\"model.h5\")\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n",
        "\n",
        "\n",
        "model.load_weights(\"model.h5\")\n",
        "model2.load_weights(\"model2 (1).h5\")\n",
        "model.layers[0].set_weights(model2.layers[0].get_weights())\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n",
        "\n",
        "model.load_weights(\"model.h5\")\n",
        "model3.load_weights(\"model33.h5\")\n",
        "model.layers[0].set_weights(model3.layers[0].get_weights())\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n",
        "\n",
        "model2.load_weights(\"model2 (1).h5\")\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model2.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n",
        "\n",
        "model.load_weights(\"model.h5\")\n",
        "model2.load_weights(\"model2 (1).h5\")\n",
        "model2.layers[0].set_weights(model.layers[0].get_weights())\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model2.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model3.load_weights(\"model33.h5\")\n",
        "model2.load_weights(\"model2 (1).h5\")\n",
        "model2.layers[0].set_weights(model3.layers[0].get_weights())\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model2.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n",
        "\n",
        "\n",
        "model3.load_weights(\"model33.h5\")\n",
        "model3.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model3.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n",
        "\n",
        "model3.load_weights(\"model33.h5\")\n",
        "model2.load_weights(\"model2 (1).h5\")\n",
        "model3.layers[0].set_weights(model2.layers[0].get_weights())\n",
        "model3.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model3.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.load_weights(\"model.h5\")\n",
        "model3.load_weights(\"model33.h5\")\n",
        "model3.layers[0].set_weights(model.layers[0].get_weights())\n",
        "model3.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy', perplexity])\n",
        "results = model3.evaluate(X,y)\n",
        "print('test loss, test acc, test per:', results)\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "204088/204088 [==============================] - 51s 249us/step\n",
            "test loss, test acc, test per: [4.633732320971615, 0.1484457684923151, 202750.71140057384]\n",
            "204088/204088 [==============================] - 52s 252us/step\n",
            "test loss, test acc, test per: [9.747747256219292, 0.013655873936733174, 2506176.3897901885]\n",
            "204088/204088 [==============================] - 52s 256us/step\n",
            "test loss, test acc, test per: [8.334856232613259, 0.014601544431813728, 979268.2633886791]\n",
            "204088/204088 [==============================] - 50s 247us/step\n",
            "test loss, test acc, test per: [5.767777129532865, 0.12091352749808809, 527884.9857842898]\n",
            "204088/204088 [==============================] - 49s 240us/step\n",
            "test loss, test acc, test per: [12.45603134204918, 0.002293128454392223, 5685731.083797186]\n",
            "204088/204088 [==============================] - 51s 251us/step\n",
            "test loss, test acc, test per: [8.408440555101336, 0.003767982438947905, 815350.6851895881]\n",
            "204088/204088 [==============================] - 53s 260us/step\n",
            "test loss, test acc, test per: [5.7399806672021825, 0.11605777899787939, 420310.28253908566]\n",
            "204088/204088 [==============================] - 55s 268us/step\n",
            "test loss, test acc, test per: [11.606718217147607, 0.014733840304182509, 4579286.980028223]\n",
            "204088/204088 [==============================] - 54s 265us/step\n",
            "test loss, test acc, test per: [13.14634169536347, 0.009495903727803692, 6585794.33781506]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gemcXWk6MDbq",
        "colab_type": "text"
      },
      "source": [
        "## Question 5:\n",
        "\n",
        "As you can see we got thr accuracy about 60%\n",
        "\n",
        "Also, after epoch ~8,9 the overfitting is possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iCdC81J395j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "bd78a5a3-3a10-484a-84cf-060c326875be"
      },
      "source": [
        "import tarfile\n",
        "tf = tarfile.open(\"20Newsgroups_subsampled.tar\")\n",
        "tf.extractall()\n",
        "\n",
        "\"\"\"This code is used to read all news and their labels\"\"\"\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def to_categories(name, cat=[\"politics\",\"rec\",\"comp\",\"religion\"]):\n",
        "    for i in range(len(cat)):\n",
        "        if str.find(name,cat[i])>-1:\n",
        "            return(i)\n",
        "    print(\"Unexpected folder: \" + name) # print the folder name which does not include expected categories\n",
        "    return(\"wth\")\n",
        "\n",
        "def data_loader(images_dir):\n",
        "    categories = os.listdir(data_path)\n",
        "    news = [] # news content\n",
        "    groups = [] # category which it belong to\n",
        "    \n",
        "    for cat in categories:\n",
        "        print(\"Category:\"+cat)\n",
        "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
        "            news.append(open(the_new_path,encoding = \"ISO-8859-1\", mode ='r').read())\n",
        "            groups.append(cat)\n",
        "\n",
        "    return news, list(map(to_categories, groups))\n",
        "\n",
        "\n",
        "\n",
        "data_path = \"20news_subsampled\"\n",
        "news, groups = data_loader(data_path)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Category:rec.sport.hockey\n",
            "Category:talk.politics.mideast\n",
            "Category:comp.windows.x\n",
            "Category:comp.os.ms-windows.misc\n",
            "Category:comp.sys.ibm.pc.hardware\n",
            "Category:comp.graphics\n",
            "Category:soc.religion.christian\n",
            "Category:talk.politics.misc\n",
            "Category:rec.autos\n",
            "Category:rec.sport.baseball\n",
            "Category:rec.motorcycles\n",
            "Category:comp.sys.mac.hardware\n",
            "Category:talk.religion.misc\n",
            "Category:talk.politics.guns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6E9xh10MG3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(news)\n",
        "\n",
        "X = []\n",
        "vocab_size = 0\n",
        "for i in range(0, len(news)):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts([news[i]])\n",
        "    encoded = tokenizer.texts_to_sequences([news[i]])[0]\n",
        "    X.extend(encoded)\n",
        "    if vocab_size < (len(tokenizer.word_index) + 1):\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = tokenizer.texts_to_sequences(wordss)\n",
        "vocab_size = len(tokenizer.index_word) +1\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, 750)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, groups, train_size=0.9,test_size=0.1)\n",
        "X_train = np.array(X_train) \n",
        "X_test = np.array(X_test)\n",
        "Y_train = to_categorical(np.array(Y_train), num_classes=4) \n",
        "Y_test = to_categorical(np.array(Y_test), num_classes=4)   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BabMJZHU4blV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "3f912b22-583d-469d-8c26-902cf5b515c5"
      },
      "source": [
        "model5 = Sequential()\n",
        "model5.add(Embedding(vocab_size, 50))\n",
        "model5.add(SimpleRNN(500, return_sequences=False, activation='sigmoid'))\n",
        "model5.add(Dense(4, activation='softmax'))\n",
        "print(model5.summary())\n",
        "\n",
        "\n",
        "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model5.fit(X_train, Y_train, epochs=15, batch_size = 64, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_18 (Embedding)     (None, None, 50)          7399950   \n",
            "_________________________________________________________________\n",
            "simple_rnn_17 (SimpleRNN)    (None, 500)               275500    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4)                 2004      \n",
            "=================================================================\n",
            "Total params: 7,677,454\n",
            "Trainable params: 7,677,454\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 11797 samples, validate on 1311 samples\n",
            "Epoch 1/15\n",
            "11797/11797 [==============================] - 67s 6ms/step - loss: 1.2593 - acc: 0.4155 - val_loss: 1.0799 - val_acc: 0.5576\n",
            "Epoch 2/15\n",
            "11797/11797 [==============================] - 65s 5ms/step - loss: 0.8025 - acc: 0.6991 - val_loss: 0.9536 - val_acc: 0.6323\n",
            "Epoch 3/15\n",
            "11797/11797 [==============================] - 65s 5ms/step - loss: 0.4156 - acc: 0.8567 - val_loss: 1.0331 - val_acc: 0.6514\n",
            "Epoch 4/15\n",
            "11797/11797 [==============================] - 65s 6ms/step - loss: 0.1766 - acc: 0.9442 - val_loss: 1.3259 - val_acc: 0.6240\n",
            "Epoch 5/15\n",
            "11797/11797 [==============================] - 65s 6ms/step - loss: 0.0851 - acc: 0.9752 - val_loss: 1.4514 - val_acc: 0.6323\n",
            "Epoch 6/15\n",
            "11797/11797 [==============================] - 65s 5ms/step - loss: 0.0567 - acc: 0.9835 - val_loss: 2.0320 - val_acc: 0.5980\n",
            "Epoch 7/15\n",
            "11797/11797 [==============================] - 65s 5ms/step - loss: 0.0421 - acc: 0.9873 - val_loss: 2.1312 - val_acc: 0.6026\n",
            "Epoch 8/15\n",
            "11797/11797 [==============================] - 65s 6ms/step - loss: 0.0382 - acc: 0.9892 - val_loss: 2.0938 - val_acc: 0.6003\n",
            "Epoch 9/15\n",
            "11797/11797 [==============================] - 64s 5ms/step - loss: 0.0247 - acc: 0.9925 - val_loss: 2.2433 - val_acc: 0.5950\n",
            "Epoch 10/15\n",
            "11797/11797 [==============================] - 64s 5ms/step - loss: 0.0226 - acc: 0.9925 - val_loss: 2.2511 - val_acc: 0.6018\n",
            "Epoch 11/15\n",
            "11797/11797 [==============================] - 64s 5ms/step - loss: 0.0847 - acc: 0.9729 - val_loss: 2.2062 - val_acc: 0.5843\n",
            "Epoch 12/15\n",
            "11797/11797 [==============================] - 64s 5ms/step - loss: 0.0271 - acc: 0.9925 - val_loss: 2.3423 - val_acc: 0.5904\n",
            "Epoch 13/15\n",
            "11797/11797 [==============================] - 65s 6ms/step - loss: 0.0184 - acc: 0.9942 - val_loss: 2.4431 - val_acc: 0.5858\n",
            "Epoch 14/15\n",
            "11797/11797 [==============================] - 65s 6ms/step - loss: 0.0163 - acc: 0.9941 - val_loss: 2.5374 - val_acc: 0.5812\n",
            "Epoch 15/15\n",
            "11797/11797 [==============================] - 65s 6ms/step - loss: 0.0170 - acc: 0.9947 - val_loss: 2.4873 - val_acc: 0.5919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3464f7358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFXSTjDAbDSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}