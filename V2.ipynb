{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "V2.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJtoMKq53943",
        "colab_type": "text"
      },
      "source": [
        "## Deep Learning Course (980)\n",
        "## Assignment Three \n",
        "\n",
        "__Assignment Goals:__\n",
        "\n",
        "- Implementing RNN based language models.\n",
        "- Implementing and applying a Recurrent Neural Network on text classification problem using TensorFlow.\n",
        "- Implementing __many to one__ and __many to many__ RNN sequence processing.\n",
        "\n",
        "In this assignment, you will implement RNN-based language models and compare extracted word representation from different models. You will also compare two different training methods for sequential data: Truncated Backpropagation Through Time __(TBTT)__ and Backpropagation Through Time __(BTT)__. \n",
        "Also, you will be asked to apply Vanilla RNN to capture word representations and solve a text classification problem. \n",
        "\n",
        "\n",
        "__DataSets__: You will use two datasets, an English Literature for language model task (part 1 to 4) and 20Newsgroups for text classification (part 5). \n",
        "\n",
        "\n",
        "1. (30 points) Implement the RNN based language model described by Mikolov et al.[1], also called __Elman network__ and train a language model on the English Literature dataset. This network contains input, hidden and output layer and is trained by standard backpropagation (TBTT with τ = 1) using the cross-entropy loss. \n",
        "   - The input represents the current word while using 1-of-N coding (thus its size is equal to the size of the vocabulary) and vector s(t − 1) that represents output values in the hidden layer from the previous time step. \n",
        "   - The hidden layer is a fully connected sigmoid layer with size 500. \n",
        "   - Softmax Output Layer to capture a valid probability distribution.\n",
        "   - The model is trained with truncated backpropagation through time (TBTT) with τ = 1: the weights of the network are updated based on the error vector computed only for the current time step.\n",
        "   \n",
        "   Download the English Literature dataset and train the language model as described, report the model cross-entropy loss on the train set. Use nltk.word_tokenize to tokenize the documents. \n",
        "For initialization, s(0) can be set to a vector of small values. Note that we are not interested in the *dynamic model* mentioned in the original paper. \n",
        "To make the implementation simpler you can use Keras to define neural net layers, including Keras.Embedding. (Keras.Embedding will create an additional mapping layer compared to the Elman architecture.) \n",
        "\n",
        "2. (20 points) TBTT has less computational cost and memory needs in comparison with *backpropagation through time algorithm (BTT)*. These benefits come at the cost of losing long term dependencies [2]. Now let's try to investigate computational costs and performance of learning our language model with BTT. For training the Elman-type RNN with BTT, one option is to perform mini-batch gradient descent with exactly one sentence per mini-batch. (The input  size will be [1, Sentence Length]). \n",
        "\n",
        "    1. Split the document into sentences (you can use nltk.tokenize.sent_tokenize).\n",
        "    2. For each sentence, perform one pass that computes the mean/sum loss for this sentence; then perform a gradient update for the whole sentence. (So the mini-batch size varies for the sentences with different lengths). You can truncate long sentences to fit the data in memory. \n",
        "    3. Report the model cross-entropy loss.\n",
        "\n",
        "3. (15 points) It does not seem that simple recurrent neural networks can capture truly exploit context information with long dependencies, because of the problem that gradients vanish and exploding. To solve this problem, gating mechanisms for recurrent neural networks were introduced. Try to learn your last model (Elman + BTT) with the SimpleRnn unit replaced with a Gated Recurrent Unit (GRU). Report the model cross-entropy loss. Compare your results in terms of cross-entropy loss with two other approach(part 1 and 2). Use each model to generate 10 synthetic sentences of 15 words each. Discuss the quality of the sentences generated - do they look like proper English? Do they match the training set?\n",
        "    Text generation from a given language model can be done using the following iterative process:\n",
        "   1. Set sequence = \\[first_word\\], chosen randomly.\n",
        "   2. Select a new word based on the sequence so far, add this word to the sequence, and repeat. At each iteration, select the word with maximum probability given the sequence so far. The trained language model outputs this probability. \n",
        "\n",
        "4. (15 points) The text describes how to extract a word representation from a trained RNN (Chapter 4). How we can evaluate the extracted word representation for your trained RNN? Compare the words representation extracted from each of the approaches using one of the existing methods.\n",
        "\n",
        "5. (20 points) We are aiming to learn an RNN model that predicts document categories given its content (text classification). For this task, we will use the 20Newsgroupst dataset. The 20Newsgroupst contains messages from twenty newsgroups.  We selected four major categories (comp, politics, rec, and religion) comprising around 13k documents altogether. Your model should learn word representations to support the classification task. For solving this problem modify the __Elman network__ architecture such that the last layer is a softmax layer with just 4 output neurons (one for each category). \n",
        "\n",
        "    1. Download the 20Newsgroups dataset, and use the implemented code from the notebook to read in the dataset.\n",
        "    2. Split the data into a training set (90 percent) and validation set (10 percent). Train the model on  20Newsgroups.\n",
        "    3. Report your accuracy results on the validation set.\n",
        "\n",
        "__NOTE__: Please use Jupyter Notebook. The notebook should include the final code, results and your answers. You should submit your Notebook in (.pdf or .html) and .ipynb format. (penalty 10 points) \n",
        "\n",
        "To reduce the parameters, you can merge all words that occur less often than a threshold into a special rare token (\\__unk__).\n",
        "\n",
        "__Instructions__:\n",
        "\n",
        "The university policy on academic dishonesty and plagiarism (cheating) will be taken very seriously in this course. Everything submitted should be your own writing or coding. You must not let other students copy your work. Spelling and grammar count.\n",
        "\n",
        "Your assignments will be marked based on correctness, originality (the implementations and ideas are from yourself), clarification and test performance.\n",
        "\n",
        "\n",
        "[1] Tom´ as Mikolov, Martin Kara ˇ fiat, Luk´ ´ as Burget, Jan ˇ Cernock´ ˇ y,Sanjeev Khudanpur: Recurrent neural network based language model, In: Proc. INTERSPEECH 2010\n",
        "\n",
        "[2] Tallec, Corentin, and Yann Ollivier. \"Unbiasing truncated backpropagation through time.\" arXiv preprint arXiv:1705.08209 (2017).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z90v3GAH3945",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"This code is used to read all news and their labels\"\"\"\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def to_categories(name, cat=[\"politics\",\"rec\",\"comp\",\"religion\"]):\n",
        "    for i in range(len(cat)):\n",
        "        if str.find(name,cat[i])>-1:\n",
        "            return(i)\n",
        "    print(\"Unexpected folder: \" + name) # print the folder name which does not include expected categories\n",
        "    return(\"wth\")\n",
        "\n",
        "def data_loader(images_dir):\n",
        "    categories = os.listdir(data_path)\n",
        "    news = [] # news content\n",
        "    groups = [] # category which it belong to\n",
        "    \n",
        "    for cat in categories:\n",
        "        print(\"Category:\"+cat)\n",
        "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
        "            news.append(open(the_new_path,encoding = \"ISO-8859-1\", mode ='r').read())\n",
        "            groups.append(cat)\n",
        "\n",
        "    return news, list(map(to_categories, groups))\n",
        "\n",
        "\n",
        "\n",
        "data_path = \"datasets/20news_subsampled\"\n",
        "news, groups = data_loader(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMoUGzkX394-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from __future__ import print_function\n",
        "# import tensorflow as tf\n",
        "# # from preeminence_utils import tf_utils\n",
        "# import numpy as np\n",
        "# import random\n",
        "# import os\n",
        "\n",
        "\n",
        "# # initialise text variables\n",
        "# data_file = \"./datasets/English Literature.txt\"\n",
        "# vocab = []\n",
        "# text = []\n",
        "# # print (text)\n",
        "# # print(len(text))\n",
        "# # vocab = sorted(list(set(text)))\n",
        "# # print(vocab)\n",
        "\n",
        "# with open('./datasets/English Literature.txt','r') as f:\n",
        "#     for line in f:\n",
        "#         for word in line.split():\n",
        "#             vocab.append(word)\n",
        "#             text.append(word)\n",
        "\n",
        "\n",
        "# #             appear = 0\n",
        "# #             if len(vocab) == 0:\n",
        "# #                 vocab.append(word)\n",
        "# #             else:\n",
        "# #                 for j in range(0, len(vocab))\n",
        "# #                     if vocab[j] == word:\n",
        "# #                         appear = 1\n",
        "# #                     if appear == 0:\n",
        "# #                         vocab.append(word)\n",
        "            \n",
        "        \n",
        "# # print (len(vocab))\n",
        "\n",
        "# vocab = set(vocab)\n",
        "\n",
        "# # print (len(vocab))\n",
        "\n",
        "\n",
        "# vocab_length = len(vocab)\n",
        "# characters2id = dict((c, i) for i, c in enumerate(vocab))\n",
        "# id2characters = dict((i, c) for i, c in enumerate(vocab))\n",
        "# section_length = 10\n",
        "# step = 10\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQNnyj_v395D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sections = []\n",
        "# section_labels = []\n",
        "# for i in range(0,len(text)-section_length,step):\n",
        "#     sections.append(text[i:i+section_length])\n",
        "#     section_labels.append(text[i+section_length])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4rt3yPh395H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_data = np.zeros((len(sections),section_length,vocab_length))\n",
        "# Y_data = np.zeros((len(sections),vocab_length))\n",
        "# for i,section in enumerate(sections):\n",
        "#     for j,letter in enumerate(section):\n",
        "#         X_data[i,j,characters2id[letter]] = 1\n",
        "#     Y_data[i,characters2id[section_labels[i]]] = 1\n",
        "\n",
        "# print(X_data.shape,Y_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nL6sMr-395K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print (Y_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McWWLOol395O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(len(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM-1uSj6395R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3cabab1b-a1d8-4974-a687-cd232975e44a"
      },
      "source": [
        "from numpy import array\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import SimpleRNN, TimeDistributed, Flatten\n",
        "import keras.backend as K\n",
        "\n",
        " \n",
        "def perplexity(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    The perplexity metric. Why isn't this part of Keras yet?!\n",
        "\n",
        "    BTW doesn't really work.\n",
        "    \"\"\"\n",
        "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
        "    perplexity = K.pow(np.e, cross_entropy)\n",
        "    return perplexity\n",
        "\n",
        "\n",
        " \n",
        "# source text\n",
        "data_file = \"./datasets/English Literature.txt\"\n",
        "\n",
        "data = open(data_file).read().strip()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "\n",
        "sequences = list()\n",
        "for i in range(1, len(encoded)):\n",
        "    sequence = encoded[i-1:i+1]\n",
        "    sequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]\n",
        "\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print (X.shape)\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 1, input_length=1))\n",
        "model.add(SimpleRNN(500, activation='sigmoid'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity])\n",
        "\n",
        "model.fit(X, y, epochs=250, batch_size=2048, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 12633\n",
            "Total Sequences: 204088\n",
            "(204088,)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1, 1)              12633     \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 500)               251000    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12633)             6329133   \n",
            "=================================================================\n",
            "Total params: 6,592,766\n",
            "Trainable params: 6,592,766\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/250\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "204088/204088 [==============================] - 209s 1ms/step - loss: 7.0788 - acc: 0.0268 - perplexity: 45871.2888\n",
            "Epoch 2/250\n",
            "204088/204088 [==============================] - 204s 1ms/step - loss: 6.8433 - acc: 0.0301 - perplexity: 24399.9721\n",
            "Epoch 3/250\n",
            "204088/204088 [==============================] - 202s 990us/step - loss: 6.8256 - acc: 0.0295 - perplexity: 23175.3079\n",
            "Epoch 4/250\n",
            "204088/204088 [==============================] - 204s 999us/step - loss: 6.7984 - acc: 0.0300 - perplexity: 22092.1753\n",
            "Epoch 5/250\n",
            "204088/204088 [==============================] - 204s 1ms/step - loss: 6.7734 - acc: 0.0300 - perplexity: 21174.8132\n",
            "Epoch 6/250\n",
            "204088/204088 [==============================] - 206s 1ms/step - loss: 6.7520 - acc: 0.0296 - perplexity: 21552.8802\n",
            "Epoch 7/250\n",
            "204088/204088 [==============================] - 206s 1ms/step - loss: 6.7337 - acc: 0.0301 - perplexity: 21895.2058\n",
            "Epoch 8/250\n",
            "184320/204088 [==========================>...] - ETA: 19s - loss: 6.7150 - acc: 0.0309 - perplexity: 21170.3623"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEhW7it395U",
        "colab_type": "code",
        "colab": {},
        "outputId": "7f765e48-248e-43a9-dad8-4aa1645f005e"
      },
      "source": [
        "# generate a sequence from the model\n",
        "def generate_seq(model, tokenizer, seed_text, n_words):\n",
        "    in_text, result = seed_text, seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        encoded = array(encoded)\n",
        "        # predict a word in the vocabulary\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text, result = out_word, result + ' ' + out_word\n",
        "    return result\n",
        " \n",
        "# def generate_seq(model, tokenizer, n_words):\n",
        "#     result = ''\n",
        "#     for _ in range(n_words):\n",
        "#         out_word = ''\n",
        "#         for word, index in tokenizer.word_index.items():\n",
        "#             out_word = word\n",
        "#             break\n",
        "#         in_text, result = out_word, result + ' ' + out_word\n",
        "#     return result\n",
        "\n",
        "# print(generate_seq(model, tokenizer, 'he', 15))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he the the the the the the the the the the the the the the the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9YaMS6D395Z",
        "colab_type": "code",
        "colab": {},
        "outputId": "0dea7499-d5c1-4036-dddd-8be7bf99edbd"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download()\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "data_file = \"./datasets/English Literature.txt\"\n",
        "\n",
        "data = open(data_file).read().strip()\n",
        "\n",
        "\n",
        "encoded = sent_tokenize(data)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(vocab_size, 1, input_length=1))\n",
        "model2.add(SimpleRNN(500, activation='sigmoid'))\n",
        "model2.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model2.summary())\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity])\n",
        "X = []\n",
        "y = []\n",
        "sequences = list()\n",
        "for i in range (0, len(encoded)):#len(encoded)\n",
        "    tokenizer.fit_on_texts([encoded[i]])\n",
        "    words = tokenizer.texts_to_sequences([encoded[i]])[0]\n",
        "    if (len(words) <= 1):\n",
        "        continue\n",
        "    for j in range(1, len(words)):\n",
        "        sequence = words[j-1:j+1]\n",
        "        sequences.append(sequence)\n",
        "#     sequences = array(sequences)\n",
        "#     X.extend(sequences[:,0])\n",
        "# #     if len(y) == 0:\n",
        "#     y.extend(sequences[:,1])\n",
        "#     else:\n",
        "#         y = y[0] + sequences[:,1]\n",
        "\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]\n",
        "\n",
        "\n",
        "# X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post')\n",
        "print(X)\n",
        "# X = X[:,0]\n",
        "# print(X.shape)\n",
        "\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)    \n",
        "model2.fit(X, y, epochs=50, verbose=1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1, 1)              12633     \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 500)               251000    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 12633)             6329133   \n",
            "=================================================================\n",
            "Total params: 6,592,766\n",
            "Trainable params: 6,592,766\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "[   1    2    3 ... 1062   25  131]\n",
            "Epoch 1/50\n",
            "191629/191629 [==============================] - 355s 2ms/step - loss: 7.0221 - accuracy: 0.0308 - perplexity: 852847.3750\n",
            "Epoch 2/50\n",
            "191629/191629 [==============================] - 359s 2ms/step - loss: 6.7593 - accuracy: 0.0350 - perplexity: 28548.5137\n",
            "Epoch 3/50\n",
            "191629/191629 [==============================] - 356s 2ms/step - loss: 6.7328 - accuracy: 0.0380 - perplexity: 26462.3164\n",
            "Epoch 4/50\n",
            "191629/191629 [==============================] - 358s 2ms/step - loss: 6.7100 - accuracy: 0.0384 - perplexity: 25297.2441\n",
            "Epoch 5/50\n",
            "191629/191629 [==============================] - 358s 2ms/step - loss: 6.6923 - accuracy: 0.0383 - perplexity: 23143.5605\n",
            "Epoch 6/50\n",
            "191629/191629 [==============================] - 359s 2ms/step - loss: 6.6783 - accuracy: 0.0382 - perplexity: 22013.6758\n",
            "Epoch 7/50\n",
            " 69952/191629 [=========>....................] - ETA: 4:03 - loss: 6.6478 - accuracy: 0.0381 - perplexity: 16326.7480"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5ad6b999fd55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    184\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[1;32m    185\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKyB7N2g395c",
        "colab_type": "code",
        "colab": {},
        "outputId": "a8441ce1-54c1-4d22-e4dc-8fed59565e2b"
      },
      "source": [
        "# print(generate_seq(model2, tokenizer, 'Jack', 15))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jack and the so the so the so the so the so the so the so\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R08LDjGb395e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import nltk\n",
        "# nltk.download()\n",
        "from keras.layers import GRU\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "data_file = \"./datasets/English Literature.txt\"\n",
        "\n",
        "data = open(data_file).read().strip()\n",
        "\n",
        "\n",
        "encoded = sent_tokenize(data)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(vocab_size, 1, input_length=1))\n",
        "model3.add(GRU(500, activation='sigmoid'))\n",
        "model3.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model3.summary())\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity])\n",
        "        \n",
        "sequences = list()\n",
        "for i in range (0, len(encoded)):#len(encoded)\n",
        "    tokenizer.fit_on_texts([encoded[i]])\n",
        "    words = tokenizer.texts_to_sequences([encoded[i]])[0]\n",
        "#     print (words)\n",
        "    if (len(words) <= 1):\n",
        "        continue\n",
        "    for j in range(1, len(words)):\n",
        "        sequence = words[j-1:j+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "    \n",
        "    \n",
        "model3.fit(X, y, epochs=400, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cpR8n7M395g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # generate a sequence from the model\n",
        "# def generate_seq(model, tokenizer, n_words):\n",
        "#     result = ''\n",
        "#     for _ in range(n_words):\n",
        "#         out_word = ''\n",
        "#         for word, index in tokenizer.word_index.items():\n",
        "#             out_word = word\n",
        "#             break\n",
        "#         in_text, result = out_word, result + ' ' + out_word\n",
        "#     return result\n",
        "\n",
        "print(generate_seq(model, tokenizer, 15))\n",
        "print(generate_seq(model2, tokenizer, 15))\n",
        "print(generate_seq(model3, tokenizer, 15))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iCdC81J395j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}